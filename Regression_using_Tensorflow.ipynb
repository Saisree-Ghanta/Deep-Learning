{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression using Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDYb_Wol8txS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDGLR5Cy9STB",
        "colab_type": "code",
        "outputId": "b489ac9b-b850-4c54-c345-5ac054315bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tf.version"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/version/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AWCw4Mh1mWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import Normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PyVKgVr9s5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x,train_y),(_,_)=tf.keras.datasets.boston_housing.load_data(test_split=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2irJ8Vm2-3f2",
        "colab_type": "code",
        "outputId": "90ebadf9-9060-4b45-c326-d33a0c842fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sa15Kb-86M",
        "colab_type": "code",
        "outputId": "aca27c3f-2a0a-4174-aee4-0e5fdec996a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_y.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02KC12Nq2BDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer=Normalizer()\n",
        "train_x=transformer.fit_transform(train_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_KFAB7D2mAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "083c3b89-b8a7-4440-f810-5683ac87c4d5"
      },
      "source": [
        "train_x[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0024119 , 0.        , 0.01592969, 0.        , 0.00105285,\n",
              "       0.01201967, 0.17945359, 0.00778265, 0.00782786, 0.6007879 ,\n",
              "       0.04109624, 0.77671895, 0.03663436])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J79SiJ9v_D3a",
        "colab_type": "code",
        "outputId": "4c6a2b25-6211-4d40-b838-daea33df4fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x.dtype"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBrwHJ-i_gd5",
        "colab_type": "text"
      },
      "source": [
        "the above data is a numpy array ,numpy array uses float64 notation to store numbers but in deep learning we use lots of data so tensorflow by default  uses 32 bit to store the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwe4YJ27Ak6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x=train_x.astype('float32')\n",
        "train_y=train_y.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5kWbq67BM-R",
        "colab_type": "text"
      },
      "source": [
        "some people even use float16 to compute more numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78N4LfDTB_n3",
        "colab_type": "text"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_aKO03jDO2n",
        "colab_type": "text"
      },
      "source": [
        "define weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nQzDJzVCEso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w=tf.zeros(shape=(13,1))\n",
        "b=tf.zeros(shape=(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yyDa3IVDXSg",
        "colab_type": "text"
      },
      "source": [
        "Define a function to calculate prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AO4U72EDd86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(x,w,b):\n",
        "  xw_matmul=tf.matmul(x,w)\n",
        "  y=tf.add(xw_matmul,b)\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGiR_sIEUYV",
        "colab_type": "text"
      },
      "source": [
        "Function to calculate loss(Mean squared Error)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DtzLA8GEgLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(y_actual,y_predicted):\n",
        "  diff=y_actual-y_predicted\n",
        "  sqr=tf.square(diff)\n",
        "  avg=tf.reduce_mean(sqr)\n",
        "  return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWAR68wjFeZc",
        "colab_type": "text"
      },
      "source": [
        "**Function to train the** **model**\n",
        "    \n",
        "\n",
        "*   Record  all the mathematical steps to calculate loss we will record all the steps using gradient tape.\n",
        "*   Calculate grdients of loss with respect to weights and bias.\n",
        "*   update weights and bias based on gradients and learning rate\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bLBYxJFGri9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x,y_actual,w,b,learning_rate=0.01):\n",
        "  with tf.GradientTape() as t:\n",
        "      t.watch([w,b])\n",
        "      current_prediction=prediction(x,w,b)\n",
        "      current_loss=loss(y_actual,current_prediction)\n",
        "  dw,db=t.gradient(current_loss,[w,b])\n",
        "  w=w-learning_rate*dw\n",
        "  b=b-learning_rate*db\n",
        "  return w,b\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwDEY4_7wtaZ",
        "colab_type": "text"
      },
      "source": [
        "Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6osxJkMJ5on",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2cf545d9-9b0a-4867-873b-9f586ee02eff"
      },
      "source": [
        "for i in range(100):\n",
        "  w,b=train(train_x,train_y,w,b,learning_rate=0.01)\n",
        "  print('current loss on iteration',i,\n",
        "        loss(train_y,prediction(train_x,w,b)).numpy())          "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current loss on iteration 0 553.7515\n",
            "current loss on iteration 1 518.26166\n",
            "current loss on iteration 2 485.45786\n",
            "current loss on iteration 3 455.13657\n",
            "current loss on iteration 4 427.1098\n",
            "current loss on iteration 5 401.20413\n",
            "current loss on iteration 6 377.25894\n",
            "current loss on iteration 7 355.12595\n",
            "current loss on iteration 8 334.66785\n",
            "current loss on iteration 9 315.75797\n",
            "current loss on iteration 10 298.27924\n",
            "current loss on iteration 11 282.1232\n",
            "current loss on iteration 12 267.1898\n",
            "current loss on iteration 13 253.38655\n",
            "current loss on iteration 14 240.62785\n",
            "current loss on iteration 15 228.83476\n",
            "current loss on iteration 16 217.93404\n",
            "current loss on iteration 17 207.85832\n",
            "current loss on iteration 18 198.54506\n",
            "current loss on iteration 19 189.9366\n",
            "current loss on iteration 20 181.9796\n",
            "current loss on iteration 21 174.62473\n",
            "current loss on iteration 22 167.82646\n",
            "current loss on iteration 23 161.54263\n",
            "current loss on iteration 24 155.73433\n",
            "current loss on iteration 25 150.36557\n",
            "current loss on iteration 26 145.40309\n",
            "current loss on iteration 27 140.81613\n",
            "current loss on iteration 28 136.5763\n",
            "current loss on iteration 29 132.65727\n",
            "current loss on iteration 30 129.0348\n",
            "current loss on iteration 31 125.68646\n",
            "current loss on iteration 32 122.59149\n",
            "current loss on iteration 33 119.73073\n",
            "current loss on iteration 34 117.086426\n",
            "current loss on iteration 35 114.64222\n",
            "current loss on iteration 36 112.38296\n",
            "current loss on iteration 37 110.29462\n",
            "current loss on iteration 38 108.36431\n",
            "current loss on iteration 39 106.58005\n",
            "current loss on iteration 40 104.93081\n",
            "current loss on iteration 41 103.406334\n",
            "current loss on iteration 42 101.9972\n",
            "current loss on iteration 43 100.6947\n",
            "current loss on iteration 44 99.49073\n",
            "current loss on iteration 45 98.37784\n",
            "current loss on iteration 46 97.34915\n",
            "current loss on iteration 47 96.39827\n",
            "current loss on iteration 48 95.51935\n",
            "current loss on iteration 49 94.70689\n",
            "current loss on iteration 50 93.955894\n",
            "current loss on iteration 51 93.2617\n",
            "current loss on iteration 52 92.620026\n",
            "current loss on iteration 53 92.02687\n",
            "current loss on iteration 54 91.478584\n",
            "current loss on iteration 55 90.97176\n",
            "current loss on iteration 56 90.50326\n",
            "current loss on iteration 57 90.070175\n",
            "current loss on iteration 58 89.66985\n",
            "current loss on iteration 59 89.299805\n",
            "current loss on iteration 60 88.957726\n",
            "current loss on iteration 61 88.6415\n",
            "current loss on iteration 62 88.34917\n",
            "current loss on iteration 63 88.07895\n",
            "current loss on iteration 64 87.82915\n",
            "current loss on iteration 65 87.59823\n",
            "current loss on iteration 66 87.38475\n",
            "current loss on iteration 67 87.187386\n",
            "current loss on iteration 68 87.00495\n",
            "current loss on iteration 69 86.83627\n",
            "current loss on iteration 70 86.68036\n",
            "current loss on iteration 71 86.5362\n",
            "current loss on iteration 72 86.40293\n",
            "current loss on iteration 73 86.27969\n",
            "current loss on iteration 74 86.16578\n",
            "current loss on iteration 75 86.060455\n",
            "current loss on iteration 76 85.96308\n",
            "current loss on iteration 77 85.87303\n",
            "current loss on iteration 78 85.78977\n",
            "current loss on iteration 79 85.71281\n",
            "current loss on iteration 80 85.6416\n",
            "current loss on iteration 81 85.57579\n",
            "current loss on iteration 82 85.514915\n",
            "current loss on iteration 83 85.45864\n",
            "current loss on iteration 84 85.406586\n",
            "current loss on iteration 85 85.35844\n",
            "current loss on iteration 86 85.31391\n",
            "current loss on iteration 87 85.27272\n",
            "current loss on iteration 88 85.23464\n",
            "current loss on iteration 89 85.199394\n",
            "current loss on iteration 90 85.16679\n",
            "current loss on iteration 91 85.13663\n",
            "current loss on iteration 92 85.108734\n",
            "current loss on iteration 93 85.08291\n",
            "current loss on iteration 94 85.05901\n",
            "current loss on iteration 95 85.03692\n",
            "current loss on iteration 96 85.01645\n",
            "current loss on iteration 97 84.99751\n",
            "current loss on iteration 98 84.97998\n",
            "current loss on iteration 99 84.96375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St8MuURZxhkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "3fec1131-baa8-4d6c-d6b0-207702f4943b"
      },
      "source": [
        "print(\"weights:\\n\",w.numpy())\n",
        "print(\"Bias:\\n\",b.numpy())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights:\n",
            " [[6.27857521e-02]\n",
            " [2.58572161e-01]\n",
            " [2.17821732e-01]\n",
            " [1.46146410e-03]\n",
            " [1.13587305e-02]\n",
            " [1.31812558e-01]\n",
            " [1.38818812e+00]\n",
            " [8.23873505e-02]\n",
            " [1.76484972e-01]\n",
            " [7.99328279e+00]\n",
            " [3.82081807e-01]\n",
            " [7.41107655e+00]\n",
            " [2.53885090e-01]]\n",
            "Bias:\n",
            " [11.476417]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}